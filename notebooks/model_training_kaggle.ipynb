{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üê± Cat Breed Classification - Kaggle Training\n",
    "\n",
    "**Optimized for Kaggle GPU Environment**\n",
    "\n",
    "This notebook implements complete training pipeline with:\n",
    "- ‚úÖ GlobalAveragePooling2D (fixed architecture)\n",
    "- ‚úÖ Two-stage training (feature extraction + fine-tuning)\n",
    "- ‚úÖ Comprehensive evaluation with confusion matrix\n",
    "- ‚úÖ Ready for Kaggle GPU (30 hours/week free)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Before Running:\n",
    "\n",
    "1. **Accelerator**: GPU P100 or T4 (Settings ‚Üí Accelerator ‚Üí GPU)\n",
    "2. **Dataset**: Attach your cat-classification dataset\n",
    "3. **Internet**: Enable if needed for packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport json\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# TensorFlow\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import ResNet50V2\nfrom tensorflow.keras.applications.resnet_v2 import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2  # Added for L2 regularization\nfrom tensorflow.keras.callbacks import (\n    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n)\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(f\"‚úì TensorFlow version: {tf.__version__}\")\nprint(f\"‚úì GPU Available: {len(tf.config.list_physical_devices('GPU'))} device(s)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration (Kaggle-optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# KAGGLE PATHS - Update these based on your dataset\n# ============================================================================\n\n# Input data (Kaggle dataset location)\nKAGGLE_INPUT = Path('/kaggle/input')\n\n# IMPORTANT: Change this to your dataset name after uploading\nDATASET_NAME = 'cat-classification-processed'  # Change to your dataset name!\nDATA_ROOT = KAGGLE_INPUT / DATASET_NAME\n\n# Data directories\nTRAIN_DIR = DATA_ROOT / 'processed' / 'train'\nVAL_DIR = DATA_ROOT / 'processed' / 'val'\nTEST_DIR = DATA_ROOT / 'processed' / 'test'\n\n# Output directory (Kaggle working directory)\nOUTPUT_DIR = Path('/kaggle/working')\nMODELS_DIR = OUTPUT_DIR / 'models'\nPLOTS_DIR = OUTPUT_DIR / 'plots'\nREPORTS_DIR = OUTPUT_DIR / 'reports'\n\n# Create output directories\nMODELS_DIR.mkdir(exist_ok=True)\nPLOTS_DIR.mkdir(exist_ok=True)\nREPORTS_DIR.mkdir(exist_ok=True)\n\n# ============================================================================\n# MODEL PARAMETERS - OPTIMIZED TO REDUCE OVERFITTING\n# ============================================================================\n\nIMG_WIDTH, IMG_HEIGHT = 224, 224\nIMG_SIZE = (IMG_WIDTH, IMG_HEIGHT)\nBATCH_SIZE = 32  # Adjust based on GPU memory\n\n# Training parameters\nEPOCHS_STAGE1 = 50  # Feature extraction\nEPOCHS_STAGE2 = 30  # Fine-tuning\nLEARNING_RATE_STAGE1 = 1e-4\nLEARNING_RATE_STAGE2 = 1e-5\n\n# Model architecture - REDUCED to prevent overfitting\nDENSE_UNITS = 256  # Reduced from 512 to 256\nDROPOUT_RATE = 0.7  # Increased from 0.5 to 0.7\nUNFREEZE_LAYERS = 30  # Reduced from 50 to 30\nL2_REG = 0.01  # L2 regularization strength\n\n# Augmentation - STRONGER to reduce overfitting\nAUGMENTATION_CONFIG = {\n    'rotation_range': 40,  # Increased from 30\n    'width_shift_range': 0.3,  # Increased from 0.2\n    'height_shift_range': 0.3,  # Increased from 0.2\n    'shear_range': 0.3,  # Increased from 0.2\n    'zoom_range': 0.3,  # Increased from 0.2\n    'horizontal_flip': True,\n    'brightness_range': [0.7, 1.3],  # Added brightness augmentation\n    'fill_mode': 'nearest'\n}\n\n# Random seed\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CONFIGURATION - ANTI-OVERFITTING OPTIMIZED\")\nprint(\"=\"*80)\nprint(f\"Data Root: {DATA_ROOT}\")\nprint(f\"Output Dir: {OUTPUT_DIR}\")\nprint(f\"Image Size: {IMG_SIZE}\")\nprint(f\"Batch Size: {BATCH_SIZE}\")\nprint(f\"Stage 1 Epochs: {EPOCHS_STAGE1} (LR: {LEARNING_RATE_STAGE1})\")\nprint(f\"Stage 2 Epochs: {EPOCHS_STAGE2} (LR: {LEARNING_RATE_STAGE2})\")\nprint(f\"\\nAnti-Overfitting Settings:\")\nprint(f\"  Dense Units: {DENSE_UNITS} (reduced)\")\nprint(f\"  Dropout Rate: {DROPOUT_RATE} (increased)\")\nprint(f\"  Unfreeze Layers: {UNFREEZE_LAYERS} (reduced)\")\nprint(f\"  L2 Regularization: {L2_REG}\")\nprint(f\"  Stronger Augmentation: ‚úì\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "print(\"Checking dataset...\\n\")\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    print(f\"‚ùå ERROR: Dataset not found at {DATA_ROOT}\")\n",
    "    print(f\"\\nAvailable datasets in /kaggle/input:\")\n",
    "    for path in KAGGLE_INPUT.iterdir():\n",
    "        print(f\"  - {path.name}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Please update DATASET_NAME variable to match your dataset!\")\n",
    "    raise FileNotFoundError(f\"Dataset not found: {DATA_ROOT}\")\n",
    "\n",
    "print(f\"‚úì Dataset found: {DATA_ROOT}\")\n",
    "\n",
    "# Check data directories\n",
    "for dir_name, dir_path in [(\"Train\", TRAIN_DIR), (\"Val\", VAL_DIR), (\"Test\", TEST_DIR)]:\n",
    "    if dir_path.exists():\n",
    "        num_breeds = len([d for d in dir_path.iterdir() if d.is_dir()])\n",
    "        print(f\"‚úì {dir_name:5s}: {dir_path} ({num_breeds} breeds)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {dir_name:5s}: NOT FOUND at {dir_path}\")\n",
    "\n",
    "# Determine number of classes\n",
    "NUM_CLASSES = len([d for d in TRAIN_DIR.iterdir() if d.is_dir()])\n",
    "print(f\"\\n‚úì Total cat breeds: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training generator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    **AUGMENTATION_CONFIG\n",
    ")\n",
    "\n",
    "# Val/Test generators without augmentation\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    str(TRAIN_DIR),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    str(VAL_DIR),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    str(TEST_DIR),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Save class indices\n",
    "class_indices = train_generator.class_indices\n",
    "with open(MODELS_DIR / 'class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úì Generators created:\")\n",
    "print(f\"  Train: {train_generator.samples} images\")\n",
    "print(f\"  Val:   {validation_generator.samples} images\")\n",
    "print(f\"  Test:  {test_generator.samples} images\")\n",
    "print(f\"  Classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Build Model (GlobalAveragePooling2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n\ndef build_model(num_classes, trainable=False):\n    \"\"\"Build model with GlobalAveragePooling2D + strong regularization\"\"\"\n    \n    # Base model\n    base_model = ResNet50V2(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n    )\n    \n    base_model.trainable = trainable\n    \n    # Build model with multiple regularization techniques\n    inputs = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n    x = base_model(inputs, training=False)\n    x = GlobalAveragePooling2D()(x)  # ‚úÖ FIXED: GAP instead of Flatten\n    \n    # First dropout layer\n    x = Dropout(0.5)(x)\n    \n    # Dense layer with L2 regularization\n    x = Dense(DENSE_UNITS, activation='relu', \n              kernel_regularizer=l2(L2_REG))(x)\n    \n    # Second dropout layer (higher rate)\n    x = Dropout(DROPOUT_RATE)(x)\n    \n    outputs = Dense(num_classes, activation='softmax')(x)\n    \n    model = Model(inputs, outputs)\n    return model\n\n# Build model\nmodel = build_model(NUM_CLASSES, trainable=False)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL SUMMARY - ANTI-OVERFITTING ARCHITECTURE\")\nprint(\"=\"*80)\nmodel.summary()\n\n# Count parameters\ntotal_params = model.count_params()\ntrainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n\nprint(f\"\\nTotal parameters: {total_params:,}\")\nprint(f\"Trainable: {trainable_params:,}\")\nprint(f\"Non-trainable: {total_params - trainable_params:,}\")\nprint(f\"\\nRegularization applied:\")\nprint(f\"  - 2x Dropout layers (0.5 + {DROPOUT_RATE})\")\nprint(f\"  - L2 regularization ({L2_REG})\")\nprint(f\"  - Reduced Dense units ({DENSE_UNITS})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Compile Model - Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE_STAGE1),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')]\n",
    ")\n",
    "\n",
    "print(\"‚úì Model compiled for Stage 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(stage=\"stage1\"):\n",
    "    \"\"\"Get training callbacks\"\"\"\n",
    "    callbacks = []\n",
    "    \n",
    "    # ModelCheckpoint\n",
    "    checkpoint_path = MODELS_DIR / f'best_{stage}.keras'\n",
    "    callbacks.append(ModelCheckpoint(\n",
    "        filepath=str(checkpoint_path),\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ))\n",
    "    \n",
    "    # EarlyStopping\n",
    "    callbacks.append(EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ))\n",
    "    \n",
    "    # ReduceLROnPlateau\n",
    "    callbacks.append(ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ))\n",
    "    \n",
    "    # CSVLogger\n",
    "    callbacks.append(CSVLogger(\n",
    "        str(REPORTS_DIR / f'training_{stage}.log'),\n",
    "        separator=',',\n",
    "        append=False\n",
    "    ))\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "callbacks_stage1 = get_callbacks(\"stage1\")\n",
    "print(\"‚úì Callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ STAGE 1: Feature Extraction (Fixed steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate steps correctly (ceiling division)\n",
    "steps_per_epoch = int(np.ceil(train_generator.samples / BATCH_SIZE))\n",
    "validation_steps = int(np.ceil(validation_generator.samples / BATCH_SIZE))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 1: FEATURE EXTRACTION (Base Frozen)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "print(f\"Epochs: {EPOCHS_STAGE1}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE_STAGE1}\\n\")\n",
    "\n",
    "# Train\n",
    "history_stage1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_STAGE1,\n",
    "    steps_per_epoch=steps_per_epoch,  # ‚úÖ FIXED\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_stage1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Stage 1 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Plot Stage 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, stage):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    if hasattr(history, 'history'):\n",
    "        history = history.history\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['loss'], label='Train')\n",
    "    axes[0].plot(history['val_loss'], label='Val')\n",
    "    axes[0].set_title(f'{stage.upper()} - Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['accuracy'], label='Train')\n",
    "    axes[1].plot(history['val_accuracy'], label='Val')\n",
    "    axes[1].set_title(f'{stage.upper()} - Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Top-5\n",
    "    if 'top5_acc' in history:\n",
    "        axes[2].plot(history['top5_acc'], label='Train')\n",
    "        axes[2].plot(history['val_top5_acc'], label='Val')\n",
    "        axes[2].set_title(f'{stage.upper()} - Top-5 Accuracy')\n",
    "        axes[2].set_xlabel('Epoch')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / f'history_{stage}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_stage1, \"stage1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü STAGE 2: Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2: FINE-TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Unfreeze top layers\n",
    "base_model = model.layers[1]\n",
    "base_model.trainable = True\n",
    "\n",
    "for layer in base_model.layers[:-UNFREEZE_LAYERS]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Recompile\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE_STAGE2),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')]\n",
    ")\n",
    "\n",
    "print(f\"Learning rate: {LEARNING_RATE_STAGE2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "callbacks_stage2 = get_callbacks(\"stage2\")\n",
    "\n",
    "history_stage2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_STAGE2,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_stage2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Stage 2 complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_stage2, \"stage2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_generator.reset()\n",
    "test_steps = int(np.ceil(test_generator.samples / BATCH_SIZE))\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_generator, steps=test_steps, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "\n",
    "test_acc = accuracy_score(true_classes, predicted_classes)\n",
    "test_top5 = top_k_accuracy_score(true_classes, predictions, k=5)\n",
    "\n",
    "print(f\"\\n‚úì Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"‚úì Test Top-5 Accuracy: {test_top5:.4f} ({test_top5*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_names)\n",
    "\n",
    "with open(REPORTS_DIR / 'classification_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Classification Report (first 20 lines):\")\n",
    "print(\"\\n\".join(report.split('\\n')[:20]))\n",
    "print(\"...\")\n",
    "print(f\"\\n‚úì Full report saved to {REPORTS_DIR / 'classification_report.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Save Final Model & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = MODELS_DIR / 'cat_breed_classifier_final.keras'\n",
    "model.save(str(final_model_path))\n",
    "print(f\"‚úì Final model saved: {final_model_path}\")\n",
    "\n",
    "# Training summary\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': 'ResNet50V2',\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'total_parameters': int(model.count_params()),\n",
    "    'stage1': {\n",
    "        'epochs': len(history_stage1.history['loss']),\n",
    "        'best_val_acc': float(max(history_stage1.history['val_accuracy'])),\n",
    "        'best_val_loss': float(min(history_stage1.history['val_loss']))\n",
    "    },\n",
    "    'stage2': {\n",
    "        'epochs': len(history_stage2.history['loss']),\n",
    "        'best_val_acc': float(max(history_stage2.history['val_accuracy'])),\n",
    "        'best_val_loss': float(min(history_stage2.history['val_loss']))\n",
    "    },\n",
    "    'test': {\n",
    "        'accuracy': float(test_acc),\n",
    "        'top5_accuracy': float(test_top5)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(REPORTS_DIR / 'training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "print(f\"‚úì Summary saved: {REPORTS_DIR / 'training_summary.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE! üéâ\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Download Instructions\n",
    "\n",
    "**All outputs are in `/kaggle/working/`:**\n",
    "\n",
    "```\n",
    "models/\n",
    "  ‚îú‚îÄ‚îÄ cat_breed_classifier_final.keras  ‚Üê Final model\n",
    "  ‚îú‚îÄ‚îÄ best_stage1.keras                 ‚Üê Best from stage 1\n",
    "  ‚îú‚îÄ‚îÄ best_stage2.keras                 ‚Üê Best from stage 2\n",
    "  ‚îî‚îÄ‚îÄ class_indices.json                ‚Üê Class mapping\n",
    "\n",
    "plots/\n",
    "  ‚îú‚îÄ‚îÄ history_stage1.png\n",
    "  ‚îú‚îÄ‚îÄ history_stage2.png\n",
    "  ‚îî‚îÄ‚îÄ confusion_matrix.png\n",
    "\n",
    "reports/\n",
    "  ‚îú‚îÄ‚îÄ training_summary.json\n",
    "  ‚îú‚îÄ‚îÄ classification_report.txt\n",
    "  ‚îú‚îÄ‚îÄ training_stage1.log\n",
    "  ‚îî‚îÄ‚îÄ training_stage2.log\n",
    "```\n",
    "\n",
    "**To download:**\n",
    "1. Click \"Save Version\" ‚Üí \"Save & Run All\"\n",
    "2. After completion: Output ‚Üí Download output files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}