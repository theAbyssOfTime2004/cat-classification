{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üê± Cat Breed Classification - Kaggle Training\n",
    "\n",
    "**Optimized for Kaggle GPU Environment**\n",
    "\n",
    "This notebook implements complete training pipeline with:\n",
    "- ‚úÖ GlobalAveragePooling2D (fixed architecture)\n",
    "- ‚úÖ Two-stage training (feature extraction + fine-tuning)\n",
    "- ‚úÖ Comprehensive evaluation with confusion matrix\n",
    "- ‚úÖ Ready for Kaggle GPU (30 hours/week free)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Before Running:\n",
    "\n",
    "1. **Accelerator**: GPU P100 or T4 (Settings ‚Üí Accelerator ‚Üí GPU)\n",
    "2. **Dataset**: Attach your cat-classification dataset\n",
    "3. **Internet**: Enable if needed for packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"‚úì TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úì GPU Available: {len(tf.config.list_physical_devices('GPU'))} device(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration (Kaggle-optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# KAGGLE PATHS - Update these based on your dataset\n",
    "# ============================================================================\n",
    "\n",
    "# Input data (Kaggle dataset location)\n",
    "KAGGLE_INPUT = Path('/kaggle/input')\n",
    "\n",
    "# IMPORTANT: Change this to your dataset name after uploading\n",
    "DATASET_NAME = 'cat-classification-processed'  # Change to your dataset name!\n",
    "DATA_ROOT = KAGGLE_INPUT / DATASET_NAME\n",
    "\n",
    "# Data directories\n",
    "TRAIN_DIR = DATA_ROOT / 'processed' / 'train'\n",
    "VAL_DIR = DATA_ROOT / 'processed' / 'val'\n",
    "TEST_DIR = DATA_ROOT / 'processed' / 'test'\n",
    "\n",
    "# Output directory (Kaggle working directory)\n",
    "OUTPUT_DIR = Path('/kaggle/working')\n",
    "MODELS_DIR = OUTPUT_DIR / 'models'\n",
    "PLOTS_DIR = OUTPUT_DIR / 'plots'\n",
    "REPORTS_DIR = OUTPUT_DIR / 'reports'\n",
    "\n",
    "# Create output directories\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "PLOTS_DIR.mkdir(exist_ok=True)\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "IMG_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "BATCH_SIZE = 32  # Adjust based on GPU memory\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS_STAGE1 = 50  # Feature extraction\n",
    "EPOCHS_STAGE2 = 30  # Fine-tuning\n",
    "LEARNING_RATE_STAGE1 = 1e-4\n",
    "LEARNING_RATE_STAGE2 = 1e-5\n",
    "\n",
    "# Model architecture\n",
    "DENSE_UNITS = 512\n",
    "DROPOUT_RATE = 0.5\n",
    "UNFREEZE_LAYERS = 50\n",
    "\n",
    "# Augmentation\n",
    "AUGMENTATION_CONFIG = {\n",
    "    'rotation_range': 30,\n",
    "    'width_shift_range': 0.2,\n",
    "    'height_shift_range': 0.2,\n",
    "    'shear_range': 0.2,\n",
    "    'zoom_range': 0.2,\n",
    "    'horizontal_flip': True,\n",
    "    'fill_mode': 'nearest'\n",
    "}\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Data Root: {DATA_ROOT}\")\n",
    "print(f\"Output Dir: {OUTPUT_DIR}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Stage 1 Epochs: {EPOCHS_STAGE1} (LR: {LEARNING_RATE_STAGE1})\")\n",
    "print(f\"Stage 2 Epochs: {EPOCHS_STAGE2} (LR: {LEARNING_RATE_STAGE2})\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "print(\"Checking dataset...\\n\")\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    print(f\"‚ùå ERROR: Dataset not found at {DATA_ROOT}\")\n",
    "    print(f\"\\nAvailable datasets in /kaggle/input:\")\n",
    "    for path in KAGGLE_INPUT.iterdir():\n",
    "        print(f\"  - {path.name}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Please update DATASET_NAME variable to match your dataset!\")\n",
    "    raise FileNotFoundError(f\"Dataset not found: {DATA_ROOT}\")\n",
    "\n",
    "print(f\"‚úì Dataset found: {DATA_ROOT}\")\n",
    "\n",
    "# Check data directories\n",
    "for dir_name, dir_path in [(\"Train\", TRAIN_DIR), (\"Val\", VAL_DIR), (\"Test\", TEST_DIR)]:\n",
    "    if dir_path.exists():\n",
    "        num_breeds = len([d for d in dir_path.iterdir() if d.is_dir()])\n",
    "        print(f\"‚úì {dir_name:5s}: {dir_path} ({num_breeds} breeds)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {dir_name:5s}: NOT FOUND at {dir_path}\")\n",
    "\n",
    "# Determine number of classes\n",
    "NUM_CLASSES = len([d for d in TRAIN_DIR.iterdir() if d.is_dir()])\n",
    "print(f\"\\n‚úì Total cat breeds: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training generator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    **AUGMENTATION_CONFIG\n",
    ")\n",
    "\n",
    "# Val/Test generators without augmentation\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    str(TRAIN_DIR),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    str(VAL_DIR),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    str(TEST_DIR),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Save class indices\n",
    "class_indices = train_generator.class_indices\n",
    "with open(MODELS_DIR / 'class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úì Generators created:\")\n",
    "print(f\"  Train: {train_generator.samples} images\")\n",
    "print(f\"  Val:   {validation_generator.samples} images\")\n",
    "print(f\"  Test:  {test_generator.samples} images\")\n",
    "print(f\"  Classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Build Model (GlobalAveragePooling2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def build_model(num_classes, trainable=False):\n",
    "    \"\"\"Build model with GlobalAveragePooling2D (fixed architecture)\"\"\"\n",
    "    \n",
    "    # Base model\n",
    "    base_model = ResNet50V2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "    )\n",
    "    \n",
    "    base_model.trainable = trainable\n",
    "    \n",
    "    # Build model\n",
    "    inputs = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)  # ‚úÖ FIXED: GAP instead of Flatten\n",
    "    x = Dense(DENSE_UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_model(NUM_CLASSES, trainable=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable: {trainable_params:,}\")\n",
    "print(f\"Non-trainable: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Compile Model - Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE_STAGE1),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')]\n",
    ")\n",
    "\n",
    "print(\"‚úì Model compiled for Stage 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(stage=\"stage1\"):\n",
    "    \"\"\"Get training callbacks\"\"\"\n",
    "    callbacks = []\n",
    "    \n",
    "    # ModelCheckpoint\n",
    "    checkpoint_path = MODELS_DIR / f'best_{stage}.keras'\n",
    "    callbacks.append(ModelCheckpoint(\n",
    "        filepath=str(checkpoint_path),\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ))\n",
    "    \n",
    "    # EarlyStopping\n",
    "    callbacks.append(EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ))\n",
    "    \n",
    "    # ReduceLROnPlateau\n",
    "    callbacks.append(ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ))\n",
    "    \n",
    "    # CSVLogger\n",
    "    callbacks.append(CSVLogger(\n",
    "        str(REPORTS_DIR / f'training_{stage}.log'),\n",
    "        separator=',',\n",
    "        append=False\n",
    "    ))\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "callbacks_stage1 = get_callbacks(\"stage1\")\n",
    "print(\"‚úì Callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ STAGE 1: Feature Extraction (Fixed steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate steps correctly (ceiling division)\n",
    "steps_per_epoch = int(np.ceil(train_generator.samples / BATCH_SIZE))\n",
    "validation_steps = int(np.ceil(validation_generator.samples / BATCH_SIZE))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 1: FEATURE EXTRACTION (Base Frozen)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "print(f\"Epochs: {EPOCHS_STAGE1}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE_STAGE1}\\n\")\n",
    "\n",
    "# Train\n",
    "history_stage1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_STAGE1,\n",
    "    steps_per_epoch=steps_per_epoch,  # ‚úÖ FIXED\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_stage1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Stage 1 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Plot Stage 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, stage):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    if hasattr(history, 'history'):\n",
    "        history = history.history\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['loss'], label='Train')\n",
    "    axes[0].plot(history['val_loss'], label='Val')\n",
    "    axes[0].set_title(f'{stage.upper()} - Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['accuracy'], label='Train')\n",
    "    axes[1].plot(history['val_accuracy'], label='Val')\n",
    "    axes[1].set_title(f'{stage.upper()} - Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Top-5\n",
    "    if 'top5_acc' in history:\n",
    "        axes[2].plot(history['top5_acc'], label='Train')\n",
    "        axes[2].plot(history['val_top5_acc'], label='Val')\n",
    "        axes[2].set_title(f'{stage.upper()} - Top-5 Accuracy')\n",
    "        axes[2].set_xlabel('Epoch')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / f'history_{stage}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_stage1, \"stage1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü STAGE 2: Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2: FINE-TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Unfreeze top layers\n",
    "base_model = model.layers[1]\n",
    "base_model.trainable = True\n",
    "\n",
    "for layer in base_model.layers[:-UNFREEZE_LAYERS]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Recompile\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE_STAGE2),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')]\n",
    ")\n",
    "\n",
    "print(f\"Learning rate: {LEARNING_RATE_STAGE2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "callbacks_stage2 = get_callbacks(\"stage2\")\n",
    "\n",
    "history_stage2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_STAGE2,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_stage2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Stage 2 complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_stage2, \"stage2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_generator.reset()\n",
    "test_steps = int(np.ceil(test_generator.samples / BATCH_SIZE))\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_generator, steps=test_steps, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "\n",
    "test_acc = accuracy_score(true_classes, predicted_classes)\n",
    "test_top5 = top_k_accuracy_score(true_classes, predictions, k=5)\n",
    "\n",
    "print(f\"\\n‚úì Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"‚úì Test Top-5 Accuracy: {test_top5:.4f} ({test_top5*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_names)\n",
    "\n",
    "with open(REPORTS_DIR / 'classification_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Classification Report (first 20 lines):\")\n",
    "print(\"\\n\".join(report.split('\\n')[:20]))\n",
    "print(\"...\")\n",
    "print(f\"\\n‚úì Full report saved to {REPORTS_DIR / 'classification_report.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Save Final Model & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = MODELS_DIR / 'cat_breed_classifier_final.keras'\n",
    "model.save(str(final_model_path))\n",
    "print(f\"‚úì Final model saved: {final_model_path}\")\n",
    "\n",
    "# Training summary\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': 'ResNet50V2',\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'total_parameters': int(model.count_params()),\n",
    "    'stage1': {\n",
    "        'epochs': len(history_stage1.history['loss']),\n",
    "        'best_val_acc': float(max(history_stage1.history['val_accuracy'])),\n",
    "        'best_val_loss': float(min(history_stage1.history['val_loss']))\n",
    "    },\n",
    "    'stage2': {\n",
    "        'epochs': len(history_stage2.history['loss']),\n",
    "        'best_val_acc': float(max(history_stage2.history['val_accuracy'])),\n",
    "        'best_val_loss': float(min(history_stage2.history['val_loss']))\n",
    "    },\n",
    "    'test': {\n",
    "        'accuracy': float(test_acc),\n",
    "        'top5_accuracy': float(test_top5)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(REPORTS_DIR / 'training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "print(f\"‚úì Summary saved: {REPORTS_DIR / 'training_summary.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE! üéâ\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Download Instructions\n",
    "\n",
    "**All outputs are in `/kaggle/working/`:**\n",
    "\n",
    "```\n",
    "models/\n",
    "  ‚îú‚îÄ‚îÄ cat_breed_classifier_final.keras  ‚Üê Final model\n",
    "  ‚îú‚îÄ‚îÄ best_stage1.keras                 ‚Üê Best from stage 1\n",
    "  ‚îú‚îÄ‚îÄ best_stage2.keras                 ‚Üê Best from stage 2\n",
    "  ‚îî‚îÄ‚îÄ class_indices.json                ‚Üê Class mapping\n",
    "\n",
    "plots/\n",
    "  ‚îú‚îÄ‚îÄ history_stage1.png\n",
    "  ‚îú‚îÄ‚îÄ history_stage2.png\n",
    "  ‚îî‚îÄ‚îÄ confusion_matrix.png\n",
    "\n",
    "reports/\n",
    "  ‚îú‚îÄ‚îÄ training_summary.json\n",
    "  ‚îú‚îÄ‚îÄ classification_report.txt\n",
    "  ‚îú‚îÄ‚îÄ training_stage1.log\n",
    "  ‚îî‚îÄ‚îÄ training_stage2.log\n",
    "```\n",
    "\n",
    "**To download:**\n",
    "1. Click \"Save Version\" ‚Üí \"Save & Run All\"\n",
    "2. After completion: Output ‚Üí Download output files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
